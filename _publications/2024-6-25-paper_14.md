---
title: "VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation"
collection: publications
permalink: /publication/2024-6-25-paper_14
excerpt: "As large language models achieve impressive scores on traditional benchmarks, an increasing number of researchers are becoming concerned about benchmark data leakage during pre-training, commonly known as the data contamination problem. To ensure fair evaluation, recent benchmarks release only the training and validation sets, keeping the test set labels closed-source. They require anyone wishing to evaluate his language model to submit the model's predictions for centralized processing and then publish the model's result on their leaderboard. However, this submission process is inefficient and prevents effective error analysis. To address this issue, we propose to variabilize benchmarks and evaluate language models dynamically. Specifically, we extract variables from each test case and define a value range for each variable. For each evaluation, we sample new values from these value ranges to create unique test cases, thus ensuring a fresh evaluation each time. We applied this variable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and TruthfulQA, which cover mathematical generation and multiple-choice tasks. Our experimental results demonstrate that this approach provides a more accurate assessment of the true capabilities of language models, effectively mitigating the contamination problem."
date: 2024-6-25
venue: 'EMNLP 2024'
paperurl: 'https://arxiv.org/abs/2406.17681'
citation: '@article{Qian2024VarBenchRL,
  title={VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation},
  author={Kun Qian and Shunji Wan and Claudia Tang and Youzhi Wang and Xuanming Zhang and Maximillian Chen and Zhou Yu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.17681},
  url={https://api.semanticscholar.org/CorpusID:270711329}
}'
---

<a href='https://arxiv.org/abs/2406.17681'>Download paper here</a>

End-to-end task-oriented dialogue (TOD) systems have achieved promising performance by leveraging sophisticated natural language understanding and natural language generation capabilities of pre-trained models. This work enables the TOD systems with more flexibility through a simple cache. The cache provides the flexibility to dynamically update the TOD systems and handle both existing and unseen dialogue scenarios. Towards this end, we first fine-tune a retrieval module to effectively retrieve the most relevant information entries from the cache. We then train end-to-end TOD models that can refer to and ground on both dialogue history and retrieved information during TOD generation. The cache is straightforward to construct, and the backbone models of TOD systems are compatible with existing pre-trained generative models. Extensive experiments demonstrate the superior performance of our framework, with a notable improvement in non-empty joint goal accuracy by 6.7% compared to strong baselines.

Recommended citation: @article{Qian2024VarBenchRL,
  title={VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation},
  author={Kun Qian and Shunji Wan and Claudia Tang and Youzhi Wang and Xuanming Zhang and Maximillian Chen and Zhou Yu},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.17681},
  url={https://api.semanticscholar.org/CorpusID:270711329}
}